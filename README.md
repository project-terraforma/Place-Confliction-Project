# Place-Confliction-Project

## ‚≠ê Overview
This project evaluates and compares multiple open-source Large Language Models (LLMs) for the task of place-record conflation using the Overture Maps 3K labeled place-pair dataset.
The goal is to determine which model‚Äîconsidering accuracy, AUC performance, and runtime efficiency‚Äîoffers the best trade-off for real-world place deduplication.
This work is part of the CROWN 102 Overture Maps Foundation partnership, emphasizing open, reproducible, and cost-efficient evaluation rather than commercial API calls.

## üßπ Dataset Pipeline
### Step 1 ‚Äî Load and inspect data
- ReadData.py loads the Overture Maps dataset from parquet format and handles:
- JSON field normalization
- Address parsing
- Category extraction
- Positive/negative label creation

### Step 2 ‚Äî Clean to CSV
- CreatCSV.py extracts:
- name
- category
- freeform address
- phone
- website for both candidate and base records.

### Step 3 ‚Äî Generate SFT JSONL
- make_sft_jsonl.py produces:
- train.jsonl
- val.jsonl
- test.jsonl
    with reversible A/B pair augmentation.

## ü§ñ Models Evaluated (Current Progress)
### ‚úÖ 1. Qwen-2 7B Instruct (Fine-Tuned)
- Fine-tuning completed early.
- Strong classification:
  - Accuracy ‚âà 0.918
  - F1 ‚âà 0.9316
- Pending: Add AUC-ROC, PR-AUC, and latency.

## ‚ö†Ô∏è 2. Qwen-3 1.7B Zero-Shot
- Performed poorly on class 0:
  - Acc ‚âà 0.60
  - F1 ‚âà 0.388
  - AUC ‚âà 0.582
- Conclusion: Model too small; no instruct version ‚Üí Fine Tuning removed from refined OKRs.

## ‚úÖ 3. Qwen-3 4B Instruct (Zero-Shot)
- Most complete results so far:
  - Accuracy ‚âà 0.728
  - F1 ‚âà 0.783
  - AUC-ROC ‚âà 0.7802
  - PR-AUC ‚âà 0.8286
- Includes ROC and PR curves.

## ‚è≥ 4. Qwen-3 4B Instruct (Fine-Tuned)
- Next model to train with QLoRA.
- Expected to outperform 4B zero-shot while being more efficient than 7B FT.

## üìä Evaluation Metrics
Each model is evaluated using:
```
| Metric                 | Description               |
|:-----------------------|:--------------------------|
| Accuracy               | Overall correctness       |
|Precision / Recall / F1 |Balanced binary evaluation |
|AUC-ROC                 | Ranking quality in imbalanced data |
|PR-AUC                  | Precision‚Äìrecall performance |
|Latency                 | Inference speed per 1,000 predictions (to be added)  |

```
Generated visualizations:
- ROC Curve
- PR Curve
- Confusion Matrix

## üìà Current Results Snapshot
```
| Model            | Fine-Tuned? | Accuracy | Recall   | F1-score | AUC-ROC | PR-AUC | Notes |
|------------------|-------------|----------|----------|----------|---------|--------|----------|
| Qwen-2 7B        | Yes         | 0.9175   | 0.9361   | 0.9316   | TBA     | TBA    | Strongest so far |
| Qwen-3 4B        | Zero-Shot   | 0.7200   | 0.8458   | 0.7838   | 0.7892  | 0.8268 | Most complete metrics |
| Qwen-3 1.7B      | Zero-Shot   | 0.6008   | 0.9917   | 0.7488   | 0.5820  | 0.6528 | Weak baseline |
```

## üîß Next Steps
### Short-Term
- Fine-tune Qwen3-4B-Instruct
- Add AUC-ROC, PR-AUC, and latency to:
  - Qwen2-7B FT
  - Qwen3-1.7B zero-shot

### Medium-Term
- Produce combined comparison tables
- Finalize ROC/PR curves for all models
- Perform ‚â•10-sample error analysis
- Prepare slide deck + final report
